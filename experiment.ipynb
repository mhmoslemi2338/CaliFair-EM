{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import os  \n",
    "import pandas as pd\n",
    "from fairness import * \n",
    "import numpy as np  \n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Try to load previously saved Data and sens_attr_dict objects from disk\n",
    "try:\n",
    "    # Load Data object containing pre-processed datasets\n",
    "    with open('Data.pkl', 'rb') as f:\n",
    "        Data = pickle.load(f)\n",
    "    # Load sens_attr_dict containing sensitive attribute information\n",
    "    with open('sens_attr_dict.pkl', 'rb') as f:\n",
    "        sens_attr_dict = pickle.load(f)\n",
    "\n",
    "# If loading fails (e.g., the files do not exist), proceed to manually process and generate these objects\n",
    "except:\n",
    "    Data = []  # Initialize an empty list to store processed data\n",
    "    result = []  # Temporary list to store intermediate results\n",
    "\n",
    "    # Iterate over files in the SCORES directory\n",
    "    for row in os.listdir('SCORES'):\n",
    "        # Skip system files\n",
    "        if 'DS_Store' not in row:\n",
    "            # Extract model and dataset names from the filename\n",
    "            model, dataset = row.replace('score_', '').rstrip('.csv').split('_', 1)\n",
    "            # For certain models, process the data differently\n",
    "            if model not in ['SVM', 'LogReg', 'LinReg']:\n",
    "                # Load true labels from the dataset\n",
    "                y_true = np.array(pd.read_csv('DATA/' + dataset + '/test.csv')['label'])\n",
    "                # Load the dataset into a DataFrame\n",
    "                df = pd.read_csv('DATA/' + dataset + '/test.csv')\n",
    "                # Load scores from the SCORES directory\n",
    "                score = pd.read_csv('SCORES/' + row)\n",
    "                score = np.array(score[score.columns[0]])\n",
    "            else:\n",
    "                # For specified datasets and models, handle differently\n",
    "                if dataset == 'DBLP-GoogleScholar':\n",
    "                    continue\n",
    "                df_ = pd.read_csv('SCORES/' + row)\n",
    "                y_true = np.array(df_['label']).reshape(-1)\n",
    "                score = np.array(df_['score']).reshape(-1)\n",
    "                # Process the DataFrame to rename columns based on sensitive attributes\n",
    "                df_ = df_[['left', 'right']]\n",
    "                df = df_.rename(columns={'left': 'left_' + sens_dict[dataset][0], 'right': 'right_' + sens_dict[dataset][0]})\n",
    "            # Append the processed scores, true labels, model, dataset, and DataFrame to the result list\n",
    "            result.append([score, y_true, model, dataset, df])\n",
    "\n",
    "    sens_attr_dict = {}  # Initialize an empty dictionary for sensitive attributes\n",
    "\n",
    "    # Iterate over the results to fill Data and sens_attr_dict\n",
    "    for i, row in enumerate(result):\n",
    "        dataset = row[-2]  # Extract dataset name\n",
    "        df = row[-1]  # Extract DataFrame\n",
    "        # If dataset is not in sens_attr_dict, create a sensitive attribute vector\n",
    "        if dataset not in list(sens_attr_dict.keys()):\n",
    "            sens_attr = make_sens_vector(df, dataset, sens_dict)\n",
    "            sens_attr_dict[dataset] = sens_attr\n",
    "        else:\n",
    "            sens_attr = sens_attr_dict[dataset]\n",
    "            # If the sensitive attribute vector does not match score shape, recreate it\n",
    "            if sens_attr.shape[0] != score.shape[0]:\n",
    "                sens_attr = make_sens_vector(df, dataset, sens_dict)\n",
    "        # Append the score, true labels, sensitive attributes, model, and dataset to the Data list\n",
    "        Data.append([row[0], row[1], sens_attr, row[2], row[3]])\n",
    "\n",
    "    # Save the sens_attr_dict to disk for future use\n",
    "    with open('sens_attr_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(sens_attr_dict, f)\n",
    "    # Save the processed Data to disk for future use\n",
    "    with open('Data.pkl', 'wb') as f:\n",
    "        pickle.dump(Data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for model and dataset to be analyzed\n",
    "MODEL = 'LogReg'\n",
    "DATASET = 'Amazon-Google'\n",
    "\n",
    "# Filter Data for the specific model and dataset\n",
    "for row in Data:\n",
    "    [score, y_true, sens_attr, model, dataset] = row\n",
    "    if dataset == DATASET and model == MODEL:\n",
    "        # Partition scores based on sensitive attribute\n",
    "        score_g1 = score[sens_attr == 1]\n",
    "        score_g2 = score[sens_attr == 0]\n",
    "        break\n",
    "\n",
    "# Initialize lists to store fairness metrics over different thresholds\n",
    "E_opp__sens = []\n",
    "E_opp__non_sens = []\n",
    "E_odds_sens = []\n",
    "E_odds__non_sens = []\n",
    "\n",
    "# Define range of thresholds to evaluate\n",
    "start, end, step = 0, 1, 100\n",
    "range = np.linspace(start, end, step)\n",
    "\n",
    "# Calculate fairness metrics for each threshold\n",
    "for TH in range:\n",
    "    # Generate predictions based on threshold\n",
    "    y_pred = np.array([1 if score > TH else 0 for score in score])\n",
    "    # Calculate fairness metrics\n",
    "    additional_fairness_metrics = calculate_additional_fairness_metrics2(y_true, y_pred, sens_attr)\n",
    "    # Extract and store each metric\n",
    "    E_opp__sens.append(additional_fairness_metrics[0]['e_opp__sens'])\n",
    "    E_opp__non_sens.append(additional_fairness_metrics[0]['e_opp__non_sens'])\n",
    "    E_odds_sens.append(additional_fairness_metrics[0]['e_odds_sens'])\n",
    "    E_odds__non_sens.append(additional_fairness_metrics[0]['e_odds__non_sens'])\n",
    "\n",
    "# Compute ROC curves and AUC scores for each group\n",
    "fpr1, tpr1, _ = roc_curve(y_true[sens_attr == 1], score_g1, drop_intermediate=False)\n",
    "fpr2, tpr2, _ = roc_curve(y_true[sens_attr == 0], score_g2, drop_intermediate=False)\n",
    "auc1 = roc_auc_score(y_true[sens_attr == 1], score_g1)\n",
    "auc2 = roc_auc_score(y_true[sens_attr == 0], score_g2)\n",
    "\n",
    "# Calculate disparity in true positive rates\n",
    "EO_opps_dist = calc_DP_TPR(sens_attr, y_true, score)\n",
    "\n",
    "# Plot settings and color definitions\n",
    "minority_col, majority_col, green_color = \"#FF5733\", \"#33AFFF\", \"#C7E9B4\"\n",
    "L, F, F_legend, F_title, size = 1.5, 28, 22, 32, (8, 6)\n",
    "\n",
    "# Plot Equal Opportunity metric\n",
    "plt.figure(figsize=size)\n",
    "plt.plot(range, E_opp__sens, label='Minority', color=minority_col)\n",
    "plt.plot(range, E_opp__non_sens, label='Majority', color=majority_col)\n",
    "plt.fill_between(range, E_opp__sens, E_opp__non_sens, color='#C0C0C0', alpha=0.3)\n",
    "plt.xlabel('Threshold (' + r'$\\tau$' + ')', fontsize=F_title)\n",
    "plt.ylabel('TPR', fontsize=F_title)\n",
    "plt.legend(fontsize=F_legend).get_frame().set_edgecolor('black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/introduction/Intro_EQ_OPP.pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot ROC curve and fill areas under curves\n",
    "plt.figure(figsize=size)\n",
    "plt.plot(fpr1, tpr1, label='Minority', color=minority_col, linewidth=L)\n",
    "plt.plot(fpr2, tpr2, label='Majority', color=majority_col, linewidth=L)\n",
    "plt.fill_between(fpr1, tpr1, color=minority_col, alpha=0.2)\n",
    "plt.fill_between(fpr2, tpr2, color=majority_col, alpha=0.2)\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--')  # Diagonal line\n",
    "plt.xlabel('FPR', fontsize=F_title)\n",
    "plt.ylabel('TPR', fontsize=F_title)\n",
    "plt.legend(fontsize=F_legend).get_frame().set_edgecolor('black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/introduction/Intro_AUC.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color scheme for plotting and groupings for fairness metrics\n",
    "\n",
    "color_dict = {'minority':\"#FF5733\",\n",
    "              'majority': \"#33AFFF\",\n",
    "              'total':'#006400'}\n",
    "G_dict = {'ACC':['minority','majority'],\n",
    "          'AUC':['minority','majority'],\n",
    "          'Equalized opportunity':['majority','minority'],\n",
    "          'Equalized odds':['minority','majority'],\n",
    "          'F1-score':['minority','majority'],\n",
    "          'Positive Rate':['majority','minority'],\n",
    "          }\n",
    "df =[]\n",
    "\n",
    "# Iterate through each data point to calculate fairness and performance metrics\n",
    "for row in Data:\n",
    "    [score, y_true,sens_attr ,model,dataset] = row\n",
    "    # Split scores by sensitive attribute\n",
    "    score_g1 = score[sens_attr ==1]\n",
    "    score_g2 = score[sens_attr ==0]\n",
    "\n",
    "    # Calculate AUC for both groups and overall\n",
    "    auc_g1 = roc_auc_score(y_true[sens_attr==1], score[sens_attr ==1])\n",
    "    auc_g2 = roc_auc_score(y_true[sens_attr==0], score[sens_attr ==0])\n",
    "    auc = roc_auc_score(y_true, score)\n",
    "    \n",
    "    \n",
    "    # Calculate fairness disparity metrics\n",
    "    Eodd_disp = calc_EO_disp(sens_attr, y_true, score)\n",
    "    Eop_disp = calc_DP_TPR(sens_attr, y_true, score)\n",
    "    PR_disp = calc_DP_PR(sens_attr, y_true, score)\n",
    "\n",
    "    # Predict and calculate metrics for threshold = 0.5\n",
    "    y_pred = np.array([1 if score > 0.5 else 0 for score in score])\n",
    "    \n",
    "    # F1 score, accuracy, and positive rate for minority and majority, and overall\n",
    "    f1_g1_5 = f1_score(y_true[sens_attr ==1], y_pred[sens_attr ==1])\n",
    "    f1_g2_5 = f1_score(y_true[sens_attr ==0], y_pred[sens_attr ==0])\n",
    "    f1_5 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate additional fairness metrics at threshold = 0.5\n",
    "    accuracy_g1_5 = accuracy_score(y_true[sens_attr ==1], y_pred[sens_attr ==1])\n",
    "    accuracy_g2_5 = accuracy_score(y_true[sens_attr ==0], y_pred[sens_attr ==0])\n",
    "    accuracy_5 = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Predict and calculate metrics for threshold = 0.9, following similar steps as for threshold = 0.5\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true[sens_attr ==1], y_pred[sens_attr ==1]).ravel()\n",
    "    PR_g1_5 = (tp + fp) / (tp+tn+fp+fn)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true[sens_attr ==0], y_pred[sens_attr ==0]).ravel()\n",
    "    PR_g2_5 = (tp + fp) / (tp+tn+fp+fn)\n",
    "\n",
    "    additional_fairness_metrics = calculate_additional_fairness_metrics2(y_true, y_pred, sens_attr)[0]\n",
    "    E_op_g1 = (additional_fairness_metrics['e_opp__sens'])\n",
    "    E_op_g2 = (additional_fairness_metrics['e_opp__non_sens'] )\n",
    "    E_od_g1 = (additional_fairness_metrics['e_odds_sens'])\n",
    "    E_od_g2 = (additional_fairness_metrics['e_odds__non_sens'])\n",
    "\n",
    "\n",
    "    E_op_5 = np.abs(E_op_g1 - E_op_g2)\n",
    "    E_od_5 = np.abs(E_od_g2 - E_od_g1)\n",
    "    PR_5 = np.abs(PR_g1_5 - PR_g2_5)\n",
    "    delta_auc = np.abs(auc_g1 - auc_g2)\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "    y_pred = np.array([1 if score > 0.9 else 0 for score in score])\n",
    "\n",
    "    f1_g1_9 = f1_score(y_true[sens_attr ==1], y_pred[sens_attr ==1])\n",
    "    f1_g2_9 = f1_score(y_true[sens_attr ==0], y_pred[sens_attr ==0])\n",
    "    f1_9 = f1_score(y_true, y_pred)\n",
    "\n",
    "    accuracy_g1_9 = accuracy_score(y_true[sens_attr ==1], y_pred[sens_attr ==1])\n",
    "    accuracy_g2_9 = accuracy_score(y_true[sens_attr ==0], y_pred[sens_attr ==0])\n",
    "    accuracy_9 = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true[sens_attr ==1], y_pred[sens_attr ==1]).ravel()\n",
    "    PR_g1_9 = (tp + fp) / (tp+tn+fp+fn)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true[sens_attr ==0], y_pred[sens_attr ==0]).ravel()\n",
    "    PR_g2_9 = (tp + fp) / (tp+tn+fp+fn)\n",
    "\n",
    "    additional_fairness_metrics = calculate_additional_fairness_metrics2(y_true, y_pred, sens_attr)[0]\n",
    "    E_op_g1 = (additional_fairness_metrics['e_opp__sens'])\n",
    "    E_op_g2 = (additional_fairness_metrics['e_opp__non_sens'] )\n",
    "    E_od_g1 = (additional_fairness_metrics['e_odds_sens'])\n",
    "    E_od_g2 = (additional_fairness_metrics['e_odds__non_sens'])\n",
    "\n",
    "\n",
    "\n",
    "    E_op_9 = np.abs(E_op_g1 - E_op_g2)\n",
    "    E_od_9 = np.abs(E_od_g2 - E_od_g1)\n",
    "    PR_9 = np.abs(PR_g1_9 - PR_g2_9)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    METRICS_dict = {\n",
    "    'Dataset': dataset,\n",
    "    'Model': model,\n",
    "    'Distributioanl disparity: Equal opportunity (TPR)': Eop_disp ,\n",
    "    'Distributioanl disparity: Equalized odds': Eodd_disp,\n",
    "    'Distributioanl disparity: PR': PR_disp,\n",
    "\n",
    "    'Positive Rate Parity (Threshold = 0.5)': PR_5,\n",
    "    'Equalized odds Parity (Threshold = 0.5)': E_od_5,\n",
    "    'Equal opportunity Parity (Threshold = 0.5)': E_op_5,\n",
    "    \n",
    "    'Total Accuracy (Threshold = 0.5)': accuracy_5,\n",
    "    'Minority Accuracy (Threshold = 0.5)': accuracy_g1_5,\n",
    "    'Majority Accuracy (Threshold = 0.5)': accuracy_g2_5,\n",
    "\n",
    "    'Total F1 (Threshold = 0.5)': f1_5,\n",
    "    'Minority F1 (Threshold = 0.5)': f1_g1_5,\n",
    "    'Majority F1 (Threshold = 0.5)': f1_g2_5,\n",
    "\n",
    "\n",
    "    'Positive Rate Parity (Threshold = 0.9)': PR_9,\n",
    "    'Equalized odds Parity (Threshold = 0.9)': E_od_9,\n",
    "    'Equal opportunity Parity (Threshold = 0.9)': E_op_9,\n",
    "    \n",
    "    'Total Accuracy (Threshold = 0.9)': accuracy_9,\n",
    "    'Minority Accuracy (Threshold = 0.9)': accuracy_g1_9,\n",
    "    'Majority Accuracy (Threshold = 0.9)': accuracy_g2_9,\n",
    "\n",
    "    'Total F1 (Threshold = 0.9)': f1_9,\n",
    "    'Minority F1 (Threshold = 0.9)': f1_g1_9,\n",
    "    'Majority F1 (Threshold = 0.9)': f1_g2_9,\n",
    "\n",
    "\n",
    "    'Total AUC': auc,\n",
    "    'Minority AUC': auc_g1,\n",
    "    'Majority AUC': auc_g2,\n",
    "    'Delta AUC': np.abs(auc_g1 - auc_g2),\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    df_new = pd.DataFrame(METRICS_dict, index=[0])\n",
    "\n",
    "\n",
    "    try:\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "    except:\n",
    "        df = copy.deepcopy(df_new)\n",
    "\n",
    "\n",
    "\n",
    "# Save the compiled metrics to a CSV file\n",
    "df.to_csv('Metric_initial.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before/After calibration Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color codes for different groups\n",
    "color_dict = {'minority':\"#FF5733\",\n",
    "              'majority': \"#33AFFF\",\n",
    "              'total':'#006400'}\n",
    "\n",
    "# Define groups for different metrics\n",
    "G_dict = {'ACC':['minority','majority'],\n",
    "          'AUC':['minority','majority'],\n",
    "          'Equalized opportunity':['majority','minority'],\n",
    "          'Equalized odds':['minority','majority'],\n",
    "          'F1-score':['minority','majority'],\n",
    "          'Positive Rate':['majority','minority'],\n",
    "          }\n",
    "\n",
    "# Initialize counters and data lists\n",
    "cnt = 0 \n",
    "df1 = []\n",
    "df2 = []\n",
    "df3 = []\n",
    "df4 = []\n",
    "\n",
    "# Loop through the data\n",
    "for row in Data:\n",
    "    # Unpack data from row\n",
    "    [score, y_true,sens_attr ,model,dataset] = row\n",
    "    score_g1 = score[sens_attr ==1]  # Scores for sensitive attribute group 1\n",
    "    score_g2 = score[sens_attr ==0]  # Scores for sensitive attribute group 2\n",
    "    \n",
    "    # Calculate barycenter Wasserstein distance\n",
    "    bary_wass, A, bin_centers1, bin_centers2 = calc_bary2(score,sens_attr, True)\n",
    "    \n",
    "    # Determine the number of bins for histograms\n",
    "    num  = min(int(max(calc_bin(score_g1), calc_bin(score_g2))), 400)\n",
    "    hist1, bin_edges1 = np.histogram(score_g1, bins=np.linspace(0, 1, num+1 ))\n",
    "    hist2, bin_edges2 = np.histogram(score_g2, bins=np.linspace(0, 1, num+1 ))\n",
    "    bin_centers1_ = 0.5 * (bin_edges1[:-1] + bin_edges1[1:])\n",
    "    bin_centers2_ = 0.5 * (bin_edges2[:-1] + bin_edges2[1:])\n",
    "    hist1 = hist1 / np.sum(hist1)\n",
    "    hist2 = hist2 / np.sum(hist2)\n",
    "\n",
    "    # Define and fit mapping transport for histograms\n",
    "    mapper1 = ot.da.MappingTransport(mu=1e-3, eta=1e-20, bias=False, max_iter=2000, verbose= False, metric = 'euclidean', tol = 1e-5)\n",
    "    mapper1.fit(Xs=hist1.reshape(-1, 1),Xt = bary_wass.reshape(-1,1))\n",
    "    mapper2 = ot.da.MappingTransport(mu=1e-3, eta=1e-20, bias=False, max_iter=2000, verbose= False, metric = 'euclidean',tol = 1e-5)\n",
    "    mapper2.fit(Xs=hist2.reshape(-1, 1), Xt =  bary_wass.reshape(-1,1))\n",
    "\n",
    "    # Transform histograms based on mappings\n",
    "    scores_list_1_mapped = mapper1.transform(Xs=hist1.reshape(-1, 1)).ravel()\n",
    "    scores_list_2_mapped = mapper2.transform(Xs=hist2.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Apply optimal transport to adjust score distributions\n",
    "    original_hist, _ = np.histogram(score_g1, bins=len(scores_list_1_mapped), range=(min(bin_edges1), max(bin_edges1)))\n",
    "    original_hist = original_hist / np.sum(original_hist)\n",
    "    target_hist = scores_list_1_mapped / np.sum(scores_list_1_mapped)\n",
    "    original_bin_midpoints = (np.linspace(min(bin_edges1), max(bin_edges1), len(original_hist))[:-1] + np.linspace(min(bin_edges1), max(bin_edges1), len(original_hist))[1:]) / 2\n",
    "    target_bin_midpoints = (bin_edges1[:-1] + bin_edges1[1:]) / 2\n",
    "    cost_matrix = ot.dist(bin_centers1[:,None], target_bin_midpoints[:, None], metric='sqeuclidean')\n",
    "    optimal_transport_plan = ot.emd(original_hist, target_hist, cost_matrix)\n",
    "    transformed_indices = np.argmax(optimal_transport_plan, axis=1)\n",
    "    transformed_data = np.interp(score_g1, bin_centers1, target_bin_midpoints[transformed_indices])\n",
    "\n",
    "    original_hist, _ = np.histogram(score_g2, bins=len(scores_list_2_mapped), range=(min(bin_edges1), max(bin_edges1)))\n",
    "    original_hist = original_hist / np.sum(original_hist)\n",
    "    target_hist = scores_list_2_mapped / np.sum(scores_list_2_mapped)\n",
    "    original_bin_midpoints = (np.linspace(min(bin_edges1), max(bin_edges1), len(original_hist))[:-1] + np.linspace(min(bin_edges1), max(bin_edges1), len(original_hist))[1:]) / 2\n",
    "    target_bin_midpoints = (bin_edges1[:-1] + bin_edges1[1:]) / 2\n",
    "    cost_matrix = ot.dist(bin_centers1[:,None], target_bin_midpoints[:, None], metric='sqeuclidean')\n",
    "    optimal_transport_plan = ot.emd(original_hist, target_hist, cost_matrix)\n",
    "    transformed_indices = np.argmax(optimal_transport_plan, axis=1)\n",
    "    transformed_data2 = np.interp(score_g2, bin_centers1, target_bin_midpoints[transformed_indices])\n",
    "\n",
    "    map_score = np.zeros(score.shape)\n",
    "    map_score[sens_attr == 1] = transformed_data \n",
    "    map_score[sens_attr == 0] = transformed_data2\n",
    "\n",
    "    # Define objective function for gamma optimization\n",
    "    def objective(gamma, score, y_true, sens_attr,score_repair):\n",
    "        score_best = score * (1-gamma) + gamma * score_repair\n",
    "        Eodd_disp = calc_EO_disp(sens_attr, y_true, score_best)\n",
    "        Eop_disp = calc_DP_TPR(sens_attr, y_true, score_best)\n",
    "        PR_disp = calc_DP_PR(sens_attr, y_true, score_best)\n",
    "        return Eodd_disp, Eop_disp, PR_disp\n",
    "\n",
    "    # Initialize lists to store objective function values\n",
    "    func_Eodd,func_PR,func_Eop = [], [] ,[]\n",
    "    \n",
    "    # Calculate objective functions for different gamma values\n",
    "    for gamma in np.linspace(0, 1, 200):\n",
    "        Eodd_disp, Eop_disp, PR_disp = objective(gamma, score, y_true, sens_attr, score_repair= map_score)\n",
    "        func_Eodd.append([gamma, Eodd_disp ])\n",
    "        func_Eop.append([gamma, Eop_disp])\n",
    "        func_PR.append([gamma, PR_disp])\n",
    "\n",
    "    # Sort the objective functions and select the optimal gamma values\n",
    "    func_Eop.sort(key= lambda x:x[1])\n",
    "    func_Eodd.sort(key= lambda x:x[1])\n",
    "    func_PR.sort(key= lambda x:x[1])\n",
    "\n",
    "    # Increment the counter and print progress\n",
    "    cnt+=1\n",
    "    print(cnt,'/',len(Data),':',MODEL+ ' '+ dataset)\n",
    "\n",
    "    # Extract the optimal gamma values\n",
    "    gamma_Eop = func_Eop[0][0]\n",
    "    gamma_Eodd = func_Eodd[0][0]\n",
    "    gamma_PR = func_PR[0][0]\n",
    "    \n",
    "    # Apply the optimal gamma values to obtain optimal scores\n",
    "    score_optimal_Eop = score * (1-gamma_Eop) + gamma_Eop * map_score\n",
    "    score_optimal_Eodd = score * (1-gamma_Eodd) + gamma_Eodd * map_score\n",
    "    score_optimal_PR = score * (1-gamma_PR) + gamma_PR * map_score\n",
    "\n",
    "    # Plot the before and after distributions\n",
    "    plot_bef_after(score_optimal_Eop,score_optimal_Eodd,score_optimal_PR, model,dataset,sens_attr, y_true, score)\n",
    "\n",
    "    # Save the metrics to dataframes\n",
    "    df1 = do_job(df1 , score_optimal_Eop,sens_attr, y_true, dataset, model, G_dict, color_dict, F, F_title, L , size , F_legend, 'final_Eop',gamma_Eop)\n",
    "    df2 = do_job(df2 , score_optimal_Eodd,sens_attr, y_true, dataset, model, G_dict, color_dict, F, F_title, L , size , F_legend, 'final_Eodd',gamma_Eodd)\n",
    "    df3 = do_job(df3 , score_optimal_PR,sens_attr, y_true, dataset, model, G_dict, color_dict, F, F_title, L , size , F_legend, 'final_PR',gamma_PR)\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "df1.to_csv('Metric_optimal_Eop.csv',index = False)\n",
    "df2.to_csv('Metric_optimal_Eodd.csv',index = False)\n",
    "df3.to_csv('Metric_optimal_PR.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
